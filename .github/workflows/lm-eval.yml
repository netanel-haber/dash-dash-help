name: lm-eval --help

on:
  workflow_dispatch:

jobs:
  bench:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v5

      - name: Install lm-eval
        run: |
          uv venv
          uv pip install lm-eval

      - name: Get version
        id: version
        run: echo "v=$(uv pip show lm-eval | grep Version | cut -d' ' -f2)" >> $GITHUB_OUTPUT

      - name: Benchmark and update
        run: |
          python3 work.py ".venv/bin/lm-eval --help" \
            --id lm-eval --library lm-eval \
            --version "${{ steps.version.outputs.v }}" \
            --version-url "https://github.com/EleutherAI/lm-evaluation-harness/releases/tag/v${{ steps.version.outputs.v }}"
