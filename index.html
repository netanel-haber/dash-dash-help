<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>llm --help</title>
  <link rel="icon" href="favicon.svg" type="image/svg+xml">
  <style>
    body {
      font-family: 'SF Mono', 'Monaco', 'Consolas', 'Courier New', monospace;
      max-width: 900px;
      margin: 2em auto;
      padding: 0 1em;
    }

    h1 {
      font-size: 1.2em;
    }

    table {
      border-collapse: collapse;
      width: 100%;
    }

    th,
    td {
      text-align: left;
      padding: 0.3em 0.5em;
      border-bottom: 1px solid #ccc;
    }

    th[aria-sort] {
      cursor: pointer;
    }

    th[aria-sort]:focus {
      outline: 2px solid currentColor;
      outline-offset: 2px;
    }

    th[aria-sort="ascending"]::after {
      content: " ▲";
    }

    th[aria-sort="descending"]::after {
      content: " ▼";
    }

    td:nth-child(5) {
      font-size: 0.85em;
    }

    td:nth-child(5) details {
      white-space: pre-line;
    }

    td:nth-child(5) summary {
      cursor: pointer;
    }

    .slow {
      color: #c00;
    }

    .ok {
      color: #080;
    }

    a {
      color: inherit;
    }

    .header {
      display: flex;
      align-items: center;
      gap: 1em;
    }

    .gh-link {
      display: inline-flex;
    }

    .gh-link svg {
      width: 20px;
      height: 20px;
    }

    @media (prefers-color-scheme: dark) {
      body {
        background: #1a1a1a;
        color: #e0e0e0;
      }

      th,
      td {
        border-bottom-color: #444;
      }

      .slow {
        color: #ff6b6b;
      }

      .ok {
        color: #51cf66;
      }
    }
  </style>
</head>

<body>
  <div class="header">
    <h1>llm --help</h1>
    <a href="https://github.com/netanel-haber/dash-dash-help" class="gh-link" title="View on GitHub">
      <svg viewBox="0 0 16 16" fill="currentColor">
        <path
          d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z" />
      </svg>
    </a>
  </div>
  <p>Let's help help help devs.</p>
  <p>Target: &lt;200ms. Most LLM CLI tools take 10+ seconds.</p>

  <table id="bench">
    <thead>
      <tr>
        <th scope="col">library</th>
        <th scope="col">command</th>
        <th scope="col">time</th>
        <th scope="col">version</th>
        <th scope="col">install</th>
        <th scope="col">pr</th>
      </tr>
    </thead>
    <tbody>
      <tr id="VLMEvalKit">
        <td>VLMEvalKit</td>
        <td><code>./VLMEvalKit/.venv/bin/python ./VLMEvalKit/run.py --help</code></td>
        <td class="slow"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20668177365">19273ms</a>
        </td>
        <td><a href="https://github.com/open-compass/VLMEvalKit/releases/tag/v0.1.0">0.1.0</a></td>
        <td>
          <details>
            <summary><code>build from source</code></summary><code>git clone https://github.com/open-compass/VLMEvalKit.git
cd VLMEvalKit
uv venv
uv pip install -e .</code>
          </details>
        </td>
        <td>-</td>
      </tr>
      <tr id="vllm">
        <td>VLLM</td>
        <td><code>.venv/bin/vllm --help</code></td>
        <td class="slow"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20669115667">16169ms</a>
        </td>
        <td><a href="https://github.com/vllm-project/vllm/releases/tag/v0.13.0">0.13.0+cpu</a></td>
        <td></td>
        <td>-</td>
      </tr>
      <tr id="sglang">
        <td>SGLang</td>
        <td><code>.venv/bin/python -m sglang.launch_server --help</code></td>
        <td class="slow"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20667880426">11343ms</a>
        </td>
        <td><a
            href="https://github.com/sgl-project/sglang/releases/tag/v0.5.6.post3.dev728+gb7c7e03d9">0.5.6.post3.dev728+gb7c7e03d9</a>
        </td>
        <td><a href="https://docs.sglang.io/platforms/cpu_server.html">CPU install</a></td>
        <td>-</td>
      </tr>
      <tr id="transformers">
        <td>Transformers</td>
        <td><code>.venv/bin/transformers-cli --help</code></td>
        <td class="slow"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20667862136">8557ms</a>
        </td>
        <td><a href="https://github.com/huggingface/transformers/releases/tag/v4.57.3">4.57.3</a></td>
        <td>
          <details>
            <summary><code>uv pip install transformers torch</code></summary><code>uv venv
uv pip install transformers torch --extra-index-url https://download.pytorch.org/whl/cpu</code>
          </details>
        </td>
        <td>-</td>
      </tr>
      <tr id="tensorrt-llm">
        <td>TensorRT-LLM</td>
        <td><code>.venv/bin/trtllm-serve --help</code></td>
        <td class="slow"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20667868764">1276ms</a>
        </td>
        <td><a href="https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v1.1.0">1.1.0</a></td>
        <td><code>uv pip install tensorrt-llm</code></td>
        <td>-</td>
      </tr>
      <tr id="hf"><td>Hugging Face Hub</td><td><code>.venv/bin/hf --help</code></td><td class="slow"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20669205055">814ms</a></td><td><a href="https://github.com/huggingface/huggingface_hub/releases/tag/v1.2.3">1.2.3</a></td><td><code>uv pip install huggingface_hub</code></td><td>-</td></tr>
      <tr id="lm-eval">
        <td>lm-eval</td>
        <td><code>.venv/bin/lm-eval --help</code></td>
        <td class="ok"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20667849112">51ms</a></td>
        <td><a href="https://github.com/EleutherAI/lm-evaluation-harness/releases/tag/v0.4.9.2">0.4.9.2</a></td>
        <td><code>uv pip install lm-eval</code></td>
        <td>-</td>
      </tr>
      <tr id="llama.cpp"><td>llama.cpp</td><td><code>./llama-bin/llama-cli --help</code></td><td class="ok"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20669207496">25ms</a></td><td><a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7616">b7616</a></td><td><a href="https://github.com/ggml-org/llama.cpp/releases">pre-built binary</a></td><td>-</td></tr>
      <tr id="ollama"><td>Ollama</td><td><code>ollama --help</code></td><td class="ok"><a href="https://github.com/netanel-haber/dash-dash-help/actions/runs/20669190350">12ms</a></td><td><a href="https://github.com/ollama/ollama/releases/tag/v0.13.5">0.13.5</a></td><td><code>curl -fsSL https://ollama.com/install.sh | sh</code></td><td>-</td></tr>
    </tbody>
  </table>
</body>

</html>